# ğŸ§  Data Citation Intent Classification

### ğŸ“˜ About the Project
This project classifies **sentences in research papers** as either:
- ğŸŸ¢ **Primary** â€” when a dataset is *used* in the research  
- ğŸ”µ **Secondary** â€” when a dataset is *only mentioned or compared*  

The model reads research papers (in XML format), extracts relevant text, and uses a **fine-tuned BERT model** to automatically identify how datasets are referenced.

ğŸ’¡ **In simple terms:**  
Instead of manually reading hundreds of research papers, this project helps identify which ones actually *use* datasets for experiments (true research content) and which ones just *mention* or *compare* them.  

For example:  
If you have **100 research papers**, the model can automatically tell you that only **20 contain real dataset usage**, and the remaining **80** are just background mentions â€” saving researchers hours of manual effort.

---

### ğŸ’¡ Real-World Use Case
In real life, this system helps:
- ğŸ› **Research organizations** (Elsevier, Springer, arXiv, etc.) automatically detect dataset usage.  
- ğŸ§¾ **Dataset creators** track where their datasets are *actually reused*.  
- ğŸ§  **Researchers** analyze dataset influence and reuse patterns.

This enables large-scale **scientific data analytics** for understanding dataset impact and research trends.

---

### ğŸ“ Folder Structure
```
D:\Data Science Projects\Data Citation Intent Classification\
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â””â”€â”€ sample_submission.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_labeled.csv
â”‚       â”œâ”€â”€ train_preprocessed.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bert_baseline_model/
â”‚   â””â”€â”€ inference_results/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_parse_xml.ipynb
â”‚   â”œâ”€â”€ 02_preprocess.ipynb
â”‚   â”œâ”€â”€ 03_baseline_model.ipynb
â”‚   â””â”€â”€ 04_bert_finetuning.ipynb
|
|
â”œâ”€â”€ Screenshots/
              â”‚
              â”œâ”€â”€ 01_project_structure.png
              â”œâ”€â”€ 02_xml_parsing.png
              â”œâ”€â”€ 03_preprocessing.png
              â”œâ”€â”€ 04_training.png
              â”œâ”€â”€ 05_text_inference.png
              â”œâ”€â”€ 06_xml_inference.png
              â”œâ”€â”€ 07_results_chart.png
              â””â”€â”€ 08_readme_preview.png
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ citation_clean.yaml
â”œâ”€â”€ inference_from_xml.py
â”œâ”€â”€ inference_text.py
â”œâ”€â”€ README.md
â”œâ”€â”€ Requirements.txt
â””â”€â”€ results_chart.py

```

---

### âš™ï¸ Installation & Setup

1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/<your-username>/Data-Citation-Intent-Classification.git
cd Data-Citation-Intent-Classification
```

2ï¸âƒ£ Create and activate your environment:
```bash
conda create -n citation_clean python=3.10
conda activate citation_clean
```

3ï¸âƒ£ Install dependencies:
```bash
pip install -r Requirements.txt
```

---

### ğŸš€ How to Run

#### ğŸ§© 1. Single Sentence Inference
For testing one example sentence:
```bash
python inference_text.py
```
Example:
```
ğŸ§  Input: We trained our model using the ImageNet dataset.
âœ… Prediction: Primary
```

#### ğŸ“„ 2. Full XML Paper Inference
Place your test XML files inside:
```
data/raw/test/XML/
```
Then run:
```bash
python inference_from_xml.py
```

Each XML fileâ€™s predictions will be saved under:
```
models/inference_results/
```

All combined results are saved as:
```
models/inference_results/predictions_from_xml.csv
```

---

### ğŸ“Š 3. Visualize Results
Run the following to plot a bar chart of predictions:
```bash
python results_chart.py
```

It will show the total number of **Primary vs Secondary** sentences classified.

ğŸ“¸ *Example Output:*  
![Results Chart](Screenshots/07_results_chart.png)

---

### ğŸ§¾ Label Descriptions
| Label | Meaning | Example |
|--------|----------|----------|
| ğŸŸ¢ **Primary (0)** | Dataset used in the paper (collected or trained upon) | â€œWe trained our model using the ImageNet dataset.â€ |
| ğŸ”µ **Secondary (1)** | Dataset only mentioned or compared | â€œWe compared our results with ImageNet.â€ |

---

### ğŸ“ˆ Results Summary
After running on **25 XML research papers**:
- Total sentences classified: **857**
- ğŸŸ¢ **Primary:** 731 (~85%)
- ğŸ”µ **Secondary:** 126 (~15%)

âœ… Model successfully extracted and classified dataset-related sentences across all test files.

---

### ğŸ§  Technologies Used
- Python ğŸ  
- Hugging Face Transformers (BERT)  
- PyTorch  
- Pandas, NumPy, Scikit-learn  
- BeautifulSoup, lxml (XML Parsing)  
- Matplotlib (Visualization)

---

### ğŸ§° Requirements
All dependencies are listed in [`Requirements.txt`](./Requirements.txt).
Install them with:
```bash
pip install -r Requirements.txt
```

---

### ğŸ“‚ Dataset
Dataset Source:  
ğŸ”— [Kaggle â€” Make Data Count: Finding Data References](https://www.kaggle.com/competitions/make-data-count-finding-data-references)

This dataset includes research papers in XML format where sentences around dataset mentions are labeled as **Primary** (dataset used) or **Secondary** (mentioned).

âš ï¸ *Due to size and license limits, dataset files are not included. Please download manually and organize as shown in the structure above.*

---

### ğŸš€ Future Scope
- Large-scale dataset usage analytics.  
- Help dataset creators track citation and reuse impact.  
- Assist research portals in dataset influence tracking.  
- Provide funding agencies insight into open-data impact.

---

### ğŸ§‘â€ğŸ’» Author
ğŸ‘¤ **Sohum Patil**  
ğŸ’¼ Aspiring Data Scientist | AI Research Enthusiast  

If you found this project useful, please â­ the repo on GitHub!

ğŸ“¬ **Contact:**  
ğŸ“§ *sohum7even@gmail.com*

---

âœ¨ *â€œTurning research data into insights â€” one sentence at a time.â€* ğŸ’–
